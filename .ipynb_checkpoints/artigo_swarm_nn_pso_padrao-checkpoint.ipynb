{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 9.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Modelo para Regressão Com backpropagation\n",
    "\n",
    "def layer_sizes2(X, Y, n_h=4):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- shape do input (quantidade de features, quantidade de exemplos)\n",
    "    Y -- shape do target (1, quantidade de exemplos)\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "def initialize_parameters2(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- tamanho da camada de entrada\n",
    "    n_h -- tamanho da camada escondida\n",
    "    n_y -- tamanho da camada de saída\n",
    "    \n",
    "    Retorna:\n",
    "    params -- dicionário com os parâmetros (pesos) iniciais do modelo:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    W1 = np.random.uniform(low = -1/np.sqrt(n_h), high = 1/np.sqrt(n_h), size = (n_h, n_x))\n",
    "    #W1 = np.random.uniform(low = -1/np.sqrt(n_h), high = 1/np.sqrt(n_h), size = (n_h, n_x)) * 0.01\n",
    "    #b1 = np.zeros((n_h, 1))\n",
    "    b1 = np.random.uniform(low = -1/np.sqrt(n_h), high = 1/np.sqrt(n_h),size = (n_h, 1))\n",
    "    W2 = np.random.uniform(low = -1/np.sqrt(n_y), high = 1/np.sqrt(n_y),size = (n_y, n_h))\n",
    "    #b2 = np.zeros((n_y, 1))\n",
    "    b2 = np.random.uniform(low = -1/np.sqrt(n_y), high = 1/np.sqrt(n_y),size = (n_y, 1))\n",
    "    \n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation2(X, parameters):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- input de tamanho (n_x, m)\n",
    "    parametros -- python dicionário contendo os parâmetros (saída da função de inicialização dos parametros)\n",
    "    \n",
    "    Retorna:\n",
    "    A2 -- A saída da função sigmoidal ou tangente hiberbólica ou relu\n",
    "    cache -- dicionário contendo \"Z1\", \"A1\", \"Z2\" e \"A2\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Recupere cada parâmetro do dicionário parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Implementando a Forward Propagation para calcular A1 tanh e A2 linear\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    #A1 = np.tanh(Z1)\n",
    "    A1 = Z1 # linear\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = 1.7159*np.tanh(2/3 * Z2)\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "\n",
    "def compute_cost2(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computa o custo dado os argumentos\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- Saída linear da segunda ativação de shape (1, qtd de exemplos)\n",
    "    Y -- Valor verdadeiro do rótulo de shape (1, qtd de exemplos)\n",
    "    parameters -- dicionário contendo os parâmetros W1, b1, W2 and b2\n",
    "    \n",
    "    Retorna:\n",
    "    cost\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # quantidade de exemplos\n",
    "\n",
    "    # Computa o custo (cost)\n",
    "    err = A2 - Y\n",
    "    cost = 1/m * np.sum(err**2)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # garanta que o custo tem a dimensão esperada\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def backward_propagation2(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implementa a retropropagação \n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário contendo os parâmetros\n",
    "    cache -- dicionário contendo \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input de shape (qtd de features, qtd de examplos)\n",
    "    Y -- valor verdadeiro do rótulo de shape (1, qtd de examplos)\n",
    "    \n",
    "    Retorna:\n",
    "    grads -- dicionário contendo os gradientes em relação aos diferentes parâmetros\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Primeiro, recuperamos W1 e W2 do dicinário \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "        \n",
    "    # Recuperamos também A1 e A2 do dicionário \"cache\".\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z2 = cache['Z2']\n",
    "    \n",
    "    # Retropropagação: calcula-se dW1, db1, dW2, db2.\n",
    "    dZ2 = (A2 - Y)* (2/3/1.7159 - np.tanh(2/3*Z2)**2)\n",
    "    #dZ2 = (A2 - Y)*(1.14393 - (A2**2)/1.5)\n",
    "    dW2 = 1/m * np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis = 1, keepdims=True)\n",
    "    #dZ1 = np.dot(W2.T, dZ2) * (1-np.power(A1, 2))\n",
    "    #dZ1 = np.dot(W2.T, dZ2) * A1\n",
    "    dZ1 = np.dot(W2.T, dZ2)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims=True)\n",
    "    \n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters2(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Atualiza os parâmetros utilizando o gradient descendente \n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário contendo os parâmetros\n",
    "    grads -- dicionário contendo os gradientes\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- dicionário contendo os parâmetros atualizados\n",
    "    \"\"\"\n",
    "    # Recupera-se cada parâmetro do dicionário \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Recupera-se cada gradiente do dicionário \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Regra de atualização para cada parâmetro\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def nn_model2(X, Y, n_h, num_iteracoes, print_cost=False):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- dataset de shape (2, qtd de examplos)\n",
    "    Y -- labels de shape (1, qtd de examplos)\n",
    "    n_h -- tamanho da camada escondida\n",
    "    num_iteracoes\n",
    "    print_cost -- se True, mostra o custo a cada 1000 iterações\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- parâmetros aprendidos pelo modelo. Eles podem ser utilizados para fazer previsões (predict).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes2(X, Y)[0]\n",
    "    n_y = layer_sizes2(X, Y)[2]\n",
    "    \n",
    "    # Inicialização dos parâmetros\n",
    "    parameters = initialize_parameters2(n_x, n_h, n_y)\n",
    "\n",
    "    # Gradiente descendente (loop)\n",
    "    for i in range(0, num_iteracoes):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation2(X, parameters)\n",
    "        \n",
    "        # Função de custo. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost2(A2, Y, parameters)\n",
    " \n",
    "        # Retropropagação (Backpropagation). Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation2(parameters, cache, X, Y)\n",
    " \n",
    "        # Atualização dos parâmetros pelo gradiente descendente. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters2(parameters, grads, learning_rate=1.2)\n",
    "        \n",
    "        # Print o custo (cost) a cada 1000 iterações\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Custo após iteração %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def predict2(parameters, X):\n",
    "    \"\"\"\n",
    "    Utiliza os parâmetros aprendidos para prever o valor da saída para cada exemplo X \n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário contendo os parâmetros\n",
    "    X -- input de tamanho (n_x, m)\n",
    "    \n",
    "    Retorna\n",
    "    predictions -- vetor de valores previstos do modelo treinado\n",
    "    \"\"\"\n",
    "    \n",
    "    A2, cache = forward_propagation2(X, parameters)\n",
    "    predictions = A2\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Resilient Backpropagation RProp ####\n",
    "\n",
    "def rprop_update(parameter, grad_old, grad_new, step_size,learning_rate_max, learning_rate_min):\n",
    "    \n",
    "    # dimensões\n",
    "    n_i = parameter.shape[0]\n",
    "    n_j = parameter.shape[1]\n",
    "\n",
    "    # transformando em vetor\n",
    "    parameter.reshape(-1, 1)\n",
    "    grad_old.reshape(-1, 1)\n",
    "    grad_new.reshape(-1, 1)\n",
    "    step_size.reshape(-1, 1)\n",
    "\n",
    "    diffs = np.multiply(grad_old, grad_new)\n",
    "    pos_indexes = np.where(diffs > 0)\n",
    "    neg_indexes = np.where(diffs < 0)\n",
    "    zero_indexes = np.where(diffs == 0)\n",
    "\n",
    "    # positive\n",
    "    if np.any(pos_indexes):\n",
    "        # calcular o peso do step size\n",
    "        step_size[pos_indexes] = np.minimum(step_size[pos_indexes] * learning_rate_max, step_size.max())       \n",
    "        # calcular a direção do peso do step size\n",
    "        grad_new[pos_indexes] = np.multiply(-np.sign(grad_new[pos_indexes]), step_size[pos_indexes])\n",
    "\n",
    "        # calcular o novo peso do parâmetro\n",
    "        parameter[pos_indexes] += grad_new[pos_indexes]\n",
    "\n",
    "\n",
    "    if np.any(neg_indexes):\n",
    "        # calcular o peso do step size\n",
    "        step_size[neg_indexes] = np.maximum(step_size[neg_indexes] * learning_rate_min, step_size.min())\n",
    "            \n",
    "        # calcular a direção do peso \n",
    "        grad_new[neg_indexes] = 0 \n",
    "\n",
    "    if np.any(zero_indexes):\n",
    "        # calcular o peso do step size\n",
    "        grad_new[zero_indexes] = np.multiply(-np.sign(grad_new[zero_indexes]), step_size[zero_indexes])\n",
    "        parameter[zero_indexes] += grad_new[zero_indexes]\n",
    "    \n",
    "\n",
    "    parameter.reshape(n_i, n_j)\n",
    "    grad_old.reshape(n_i, n_j)\n",
    "    grad_new.reshape(n_i, n_j)\n",
    "    step_size.reshape(n_i, n_j)\n",
    "\n",
    "    return parameter, grad_new, step_size\n",
    "    \n",
    "    # for i in range(n_i):\n",
    "\n",
    "    #     for j in range(n_j):\n",
    "\n",
    "    #         if grad_old[i, j] * grad_new[i, j] > 0:\n",
    "    #             step_size[i, j] = min(step_size[i,j] * learning_rate_max, step_size.max())\n",
    "    #             grad_new[i, j] = np.sign(grad_new[i, j]) * step_size[i, j]\n",
    "    #             #grad_new[i, j] = - np.sign(grad_new[i, j]) * step_size[i, j]\n",
    "    #             #parameter[i, j] = parameter[i, j] + grad_new[i, j]\n",
    "            \n",
    "    #         elif grad_old[i, j] * grad_new[i, j] < 0:\n",
    "    #             step_size[i, j] = max(step_size[i,j] * learning_rate_min, step_size.min())\n",
    "    #             grad_new[i, j] = 0\n",
    "            \n",
    "    #         else:\n",
    "    #             grad_new[i, j] = np.sign(grad_new[i, j]) * step_size[i, j] \n",
    "    #             #grad_new[i, j] = - np.sign(grad_new[i, j]) * step_size[i, j]\n",
    "    #             #parameter[i, j] = parameter[i, j] + grad_new[i, j]\n",
    "\n",
    "    # parameter = parameter - grad_new\n",
    "\n",
    "    # return parameter, grad_new, step_size\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_parameters_rprop2(parameters, grads_old, grads_new, step_size,learning_rate_max = 1.2, learning_rate_min = 0.5):\n",
    "    \"\"\"\n",
    "    Atualiza os parâmetros utilizando o Resilient backpropagation\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário contendo os parâmetros\n",
    "    grads -- dicionário contendo os gradientes\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- dicionário contendo os parâmetros atualizados\n",
    "    \"\"\"\n",
    "    # Recupera-se cada parâmetro do dicionário \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Recupera-se cada gradiente do dicionário \"grads_old\" e \"grads_new\"\n",
    "    dW1_old = grads_old['dW1']\n",
    "    db1_old = grads_old['db1']\n",
    "    dW2_old = grads_old['dW2']\n",
    "    db2_old = grads_old['db2']\n",
    "\n",
    "    dW1_new = grads_new['dW1']\n",
    "    db1_new = grads_new['db1']\n",
    "    dW2_new = grads_new['dW2']\n",
    "    db2_new = grads_new['db2']\n",
    "\n",
    "    W1_step = step_size['W1']\n",
    "    b1_step = step_size['b1']\n",
    "    W2_step = step_size['W2']\n",
    "    b2_step = step_size['b2']\n",
    "\n",
    "    W2, dW2_new, W2_step = rprop_update(W2,dW2_old, dW2_new, W2_step, learning_rate_max, learning_rate_min)\n",
    "    b2, db2_new, b2_step = rprop_update(b2,db2_old, db2_new, b2_step, learning_rate_max, learning_rate_min)\n",
    "    W1, dW1_new, W1_step = rprop_update(W1, dW1_old, dW1_new, W1_step, learning_rate_max, learning_rate_min)\n",
    "    b1, db1_new, b1_step = rprop_update(b1,db1_old, db1_new, b1_step, learning_rate_max, learning_rate_min) \n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    grads = {\"dW1\": dW1_new,\n",
    "             \"db1\": db1_new,\n",
    "             \"dW2\": dW2_new,\n",
    "             \"db2\": db2_new}\n",
    "\n",
    "    step_size = {\"W1\": W1_step,\n",
    "                 \"b1\": b1_step,\n",
    "                 \"W2\": W2_step,\n",
    "                 \"b2\": b2_step}\n",
    "\n",
    "    return parameters, grads, step_size\n",
    "\n",
    "def nn_model_rprop(X, Y, n_h, num_iteracoes, print_cost=False):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- dataset de shape (2, qtd de examplos)\n",
    "    Y -- labels de shape (1, qtd de examplos)\n",
    "    n_h -- tamanho da camada escondida\n",
    "    num_iteracoes -- quantidade de iterações do gradiente descendente\n",
    "    print_cost -- se True, mostra o custo a cada 1000 iterações\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- parâmetros aprendidos pelo modelo. Eles podem ser utilizados para fazer previsões (predict).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes2(X, Y)[0]\n",
    "    n_y = layer_sizes2(X, Y)[2]\n",
    "    \n",
    "    # Inicialização dos parâmetros\n",
    "    parameters = initialize_parameters2(n_x, n_h, n_y)\n",
    "    \n",
    "    # Gradiente descendente (loop)\n",
    "    grads_old = {\"dW1\": np.zeros((n_h, n_x)),\n",
    "                 \"db1\": np.zeros((n_h, 1)),\n",
    "                 \"dW2\": np.zeros((n_y, n_h)),\n",
    "                 \"db2\": np.zeros((n_y, 1))}\n",
    "\n",
    "    step_size = {\"W1\": np.random.rand(n_h, n_x),\n",
    "                \"b1\": np.random.rand(n_h, 1),\n",
    "                \"W2\": np.random.rand(n_y, n_h),\n",
    "                \"b2\": np.random.rand(n_y, 1)}\n",
    "\n",
    "    for i in range(0, num_iteracoes):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation2(X, parameters)\n",
    "        \n",
    "        # Função de custo. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost2(A2, Y, parameters)\n",
    " \n",
    "        # Retropropagação (Backpropagation). Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads_new = backward_propagation2(parameters, cache, X, Y)\n",
    " \n",
    "        # Atualização dos parâmetros pelo gradiente descendente. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters, grads_old, step_size = update_parameters_rprop2(parameters, grads_old, grads_new, step_size, learning_rate_max=1.2, learning_rate_min=0.5)\n",
    "\n",
    "        # Print o custo (cost) a cada 1000 iterações\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Custo após iteração %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Estruturando os dados de dicionário para numpy array e de numpy array para dicionário\n",
    "def parameter_dim_tot(parameter):\n",
    "    '''\n",
    "    Argumentos:\n",
    "    parameter - array de parâmetros\n",
    "\n",
    "    Retorna:\n",
    "    dim_tot - dimensão total dos parâmetris \n",
    "    '''\n",
    "    dim_tot = np.array(parameter.shape).prod()\n",
    "\n",
    "    return dim_tot\n",
    "\n",
    "def parameter_reshape_coluna(parameter):\n",
    "    '''\n",
    "    Argumentos:\n",
    "    parameter - array de parâmetros\n",
    "\n",
    "    Retorna:\n",
    "    parameter_reshaped - array coluna dos parâmetros\n",
    "    '''\n",
    "    param_dim_tot = parameter_dim_tot(parameter)\n",
    "    parameter_reshaped = parameter.reshape(1, param_dim_tot)\n",
    "\n",
    "    return parameter_reshaped\n",
    "\n",
    "def parameters_stack(parameters):\n",
    "    '''\n",
    "    Argumentos: \n",
    "    parameters - lista com os parâmetros em array \n",
    "\n",
    "    Retorna:\n",
    "    parametros_stack - array coluna com parâmetros empilhados\n",
    "    '''\n",
    "    params_list = []\n",
    "    param_temp = 0\n",
    "    \n",
    "    for param in parameters:\n",
    "        param_temp = parameter_reshape_coluna(param)\n",
    "        params_list.append(param_temp)\n",
    "    \n",
    "    params_stack = np.concatenate(tuple(params_list), axis = 1)\n",
    "\n",
    "    return params_stack\n",
    "\n",
    "# Unstack os parâmetros com base na dimensão dos atributos (matrizes de pesos)\n",
    "def parameters_unstack(parameters_stack, atributos_dim):\n",
    "    '''\n",
    "    Argumentos:\n",
    "    parameters_stack - array dos parâmetros no formato empilhado por colunas para trabalhar no PSO\n",
    "    atributos_dim - lista com dimensão total dos atributos \n",
    "\n",
    "    Retorna:\n",
    "    params - lista com parâmetros no formato de lista\n",
    "    '''\n",
    "    params = []\n",
    "    i = atributos_dim[0]\n",
    "    params.append(parameters_stack[:, :i])\n",
    "\n",
    "    for dim in atributos_dim[1:]:\n",
    "        params.append(parameters_stack[:, i:i+dim])\n",
    "        i += dim\n",
    "\n",
    "    return params\n",
    "\n",
    "# Reshape para o formato do dicionário (parameters)\n",
    "def parameters_reshape_dictionary(parameters_dict, parameters_unstacked):\n",
    "    '''\n",
    "    Argumentos:\n",
    "    parameters_dict - dicionário com parâmetros\n",
    "    parameters_unstacked - parâmetros no formato array \n",
    "\n",
    "    retorna:\n",
    "    parameters_reshaped - lista com os parâmetros formatados para o dicionário 'parameters'\n",
    "    '''\n",
    "    w1_shape = parameters_dict['W1'].shape\n",
    "    b1_shape = parameters_dict['b1'].shape\n",
    "    w2_shape = parameters_dict['W2'].shape\n",
    "    b2_shape = parameters_dict['b2'].shape\n",
    "\n",
    "    parameters_reshaped = parameters_dict.copy()\n",
    "\n",
    "    w1_reshaped = parameters_unstacked[0].reshape(w1_shape)\n",
    "    b1_reshaped = parameters_unstacked[1].reshape(b1_shape)\n",
    "    w2_reshaped = parameters_unstacked[2].reshape(w2_shape)\n",
    "    b2_reshaped = parameters_unstacked[3].reshape(b2_shape)\n",
    "\n",
    "    parameters_reshaped['W1'] = w1_reshaped\n",
    "    parameters_reshaped['b1'] = b1_reshaped\n",
    "    parameters_reshaped['W2'] = w2_reshaped\n",
    "    parameters_reshaped['b2'] = b2_reshaped\n",
    "\n",
    "    return parameters_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PSO para otimizar todos os parâmetros de uma só vez \n",
    "    \n",
    "def PSO_todos(X, parameters, parameters_stacked, fun, Y, best_cost,qtd_particulas, atributos_dim, min_i, max_i, \n",
    "              max_epoch, w_in=0.7, w_fim = 0.2, c1=1.496, c2=1.496):\n",
    "    '''\n",
    "        Função do Algoritmo SWARM PSO. \n",
    "        Inputs:\n",
    "        - fun_opt: Função de fitness a ser otimizada\n",
    "        - qtd_particulas: Quantidade de partículas\n",
    "        - atributos_dim: Dimensão do Vetor de atributos \n",
    "        - min: intervalo inferior do domínio da função  \n",
    "        - max: intervalo superior do domínio da função\n",
    "        - w: inércia \n",
    "        - c1: influência do pbest (termo cognitivo)\n",
    "        - c2: influência do gbest (termo do aprendizado social)\n",
    "    '''\n",
    "    \n",
    "    def weight_decay(w_in, w_fim, iter, iter_max):\n",
    "        \n",
    "        return w_in + w_fim * (1 - (iter/iter_max))\n",
    "\n",
    "\n",
    "    atributos_dim_sum = sum(atributos_dim)\n",
    "\n",
    "    # inicializar as partículas em posições aleatórias\n",
    "    particulas = np.random.uniform(low = min_i, high = max_i, size = (qtd_particulas, atributos_dim_sum))\n",
    "\n",
    "    # inicializar a velocidade\n",
    "    velocidade = np.zeros((qtd_particulas, atributos_dim_sum))\n",
    "\n",
    "    # inicializar o pbest em zero\n",
    "    pbest = np.zeros((qtd_particulas,atributos_dim_sum))\n",
    "\n",
    "    gbest_value = best_cost\n",
    "    print('Custo gbest inicio PSO = ', gbest_value)\n",
    "\n",
    "    gbest = 0\n",
    "    #particulas[gbest,:] = parameters_stacked\n",
    "    \n",
    "    parameters_gbest_dict = parameters.copy()\n",
    "    parameters_dict = parameters.copy()\n",
    "\n",
    "    # Extrair a posição do gbest \n",
    "    for z in np.arange(qtd_particulas):\n",
    "        parameters_temp = particulas[[z],:]\n",
    "        parameters_temp_unstacked = parameters_unstack(parameters_temp, atributos_dim)\n",
    "        parameters_temp_dict = parameters_reshape_dictionary(parameters_dict, parameters_temp_unstacked)\n",
    "        A2 = predict2(parameters_temp_dict, X)\n",
    "        new_value = fun(A2, Y, parameters_temp_dict)\n",
    "\n",
    "        if new_value < gbest_value:\n",
    "            gbest_value = new_value\n",
    "            gbest = z\n",
    "            parameters_gbest_dict = parameters_temp_dict\n",
    "\n",
    "    # print(parameters_gbest_dict)\n",
    "    print('gbest', gbest)\n",
    "    \n",
    "    for k in np.arange(max_epoch):\n",
    "\n",
    "        # Atualização do decaimento do peso\n",
    "        w = weight_decay(w_in, w_fim,k, max_epoch)                \n",
    "        \n",
    "    # Iterar para atualizar o pbest e gbest para cada partrícula\n",
    "        for j in np.arange(qtd_particulas):\n",
    "        \n",
    "            # transformando as partículas no formato de dicionário\n",
    "            parameters_temp = particulas[[j],:]\n",
    "            parameters_temp_unstacked = parameters_unstack(parameters_temp, atributos_dim)\n",
    "            parameters_temp_dict = parameters_reshape_dictionary(parameters_dict, parameters_temp_unstacked)\n",
    "\n",
    "            parameters_pbest_temp = pbest[[j],:]\n",
    "            parameters_pbest_temp_unstacked = parameters_unstack(parameters_pbest_temp, atributos_dim)\n",
    "            parameters_pbest_dict = parameters_reshape_dictionary(parameters_dict, parameters_temp_unstacked)\n",
    "\n",
    "            A2_part = predict2(parameters_temp_dict, X)\n",
    "            A2_pbest = predict2(parameters_pbest_dict, X)\n",
    "            \n",
    "            # pbest\n",
    "            if fun(A2_part, Y, parameters_temp_dict) < fun(A2_pbest, Y, parameters_pbest_dict):\n",
    "                pbest[j,:] = particulas[j,:]\n",
    "\n",
    "            # gbest\n",
    "            if fun(A2_part, Y, parameters_temp_dict) < gbest_value:\n",
    "                if np.abs(fun(A2_part, Y, parameters_temp_dict) - gbest_value) < 0.00001:\n",
    "                    print('Parar o algoritmo pois o gbest não melhorou')\n",
    "                    gbest_value = fun(A2_part, Y, parameters_temp_dict)\n",
    "                    gbest = j\n",
    "                    parameters_gbest_dict = parameters_temp_dict\n",
    "                    break\n",
    "                    \n",
    "                print('\\nCusto menor que Gbest na iteração ', k)\n",
    "                print('Custo = ', fun(A2_part, Y, parameters_temp_dict))\n",
    "                gbest = j\n",
    "                gbest_value = fun(A2_part, Y, parameters_temp_dict)\n",
    "                parameters_gbest_dict = parameters_temp_dict\n",
    "                                      \n",
    "         # Iteração para atualizar as posições das partículas\n",
    "        for i in np.arange(qtd_particulas):\n",
    "            r1, r2 = np.random.rand(), np.random.rand()\n",
    "            velocidade[i, :] = w * velocidade[i, :] + c1 * r1 * (pbest[i, :] - particulas[i, :]) + c2 * r2 * (particulas[gbest, :] - particulas[i, :])\n",
    "                # Atualizar partículas\n",
    "            particulas[i, :] = particulas[i, :] + velocidade[i, :]\n",
    "\n",
    "            # lidar com limites das partículas\n",
    "            for dim in np.arange(atributos_dim_sum):\n",
    "                if particulas[i, dim] < min_i:\n",
    "                    particulas[i, dim] = min_i\n",
    "                elif particulas[i, dim] > max_i:\n",
    "                    particulas[i, dim] = max_i\n",
    "\n",
    "    return parameters_gbest_dict\n",
    "\n",
    "def update_parameters_pso_todos(X, parameters, compute_cost2, Y, best_cost, qtd_iteracoes):\n",
    "    '''\n",
    "    Argumentos:\n",
    "    parameters - dicionário contendo os parâmetros do modelo\n",
    "    compute_cost2 - função a ser minimizada, neste caso a função de custo\n",
    "    A2 - previsão feita pelo modelo\n",
    "    Y - rótulo \n",
    "\n",
    "    Retorna:\n",
    "    parameters - parâmetros atualizados a partir do PSO\n",
    "    '''\n",
    "\n",
    "    # Extrair os parâmetros do dicionário para calcular a dimensão total e para criar o array colunas\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    # Extrair a dimensão total \n",
    "    W1_dim = np.array(W1.shape).prod()\n",
    "    b1_dim = np.array(b1.shape).prod()\n",
    "    W2_dim = np.array(W2.shape).prod()\n",
    "    b2_dim = np.array(b2.shape).prod()\n",
    "\n",
    "    # lista com parâmetros\n",
    "    parametros = [W1, b1, W2, b2]\n",
    "    # parâmetros no formato array colunas\n",
    "    parameters_stacked = parameters_stack(parametros)\n",
    "\n",
    "    atributos_dim = [W1_dim, b1_dim, W2_dim, b2_dim]\n",
    "\n",
    "    qtd_particulas_dim = (W1.shape[1] + 1)*W1.shape[0] + (W1.shape[0] + 1)*W2.shape[0]\n",
    "\n",
    "    parameters_pso = PSO_todos(X = X, parameters = parameters, parameters_stacked = parameters_stacked, \n",
    "                               fun = compute_cost2, Y = Y, best_cost = best_cost, qtd_particulas = qtd_particulas_dim, \n",
    "                               atributos_dim = atributos_dim, min_i = -1, max_i = 1, max_epoch = qtd_iteracoes)\n",
    "    \n",
    "    A2 = predict2(parameters_pso, X)\n",
    "    best_cost = compute_cost2(A2, Y, parameters_pso)\n",
    "    \n",
    "    print('Erro de treinamento após otimizar parâmetros = ' + str(best_cost))\n",
    "\n",
    "    return parameters_pso\n",
    "\n",
    "def nn_model_pso_todos(X, Y, n_h, num_iteracoes, print_cost=False):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- dataset de shape (2, qtd de exemplos)\n",
    "    Y -- labels de shape (1, qtd de exemplos)\n",
    "    n_h -- tamanho da camada escondida\n",
    "    num_iteracoes\n",
    "    print_cost -- se True, mostra o custo a cada 1000 iterações\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- parâmetros aprendidos pelo pso. Eles podem ser utilizados para fazer previsões (predict).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes2(X, Y)[0]\n",
    "    n_y = layer_sizes2(X, Y)[2]\n",
    "    \n",
    "    # Inicialização dos parâmetros\n",
    "    parameters = initialize_parameters2(n_x, n_h, n_y)\n",
    "    \n",
    "    A2, _ = forward_propagation2(X, parameters)\n",
    "\n",
    "    best_cost = compute_cost2(A2, Y, parameters)\n",
    "    \n",
    "    # Atualização dos parâmetros pelo gradiente descendente. Inputs: \"parameters, compute_cost2, A2, Y\". Outputs: \"parameters\".\n",
    "    parameters = update_parameters_pso_todos(X, parameters, compute_cost2, Y, best_cost, num_iteracoes)\n",
    "\n",
    "    # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "    A2, _ = forward_propagation2(X, parameters)\n",
    "\n",
    "    # Função de custo. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "    cost_new = compute_cost2(A2, Y, parameters)\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "#     for i in range(0, num_iterations):\n",
    " \n",
    "#         # Atualização dos parâmetros pelo gradiente descendente. Inputs: \"parameters, compute_cost2, A2, Y\". Outputs: \"parameters\".\n",
    "#         parameters = update_parameters_pso_todos(X, parameters, compute_cost2, Y, best_cost, num_iterations)\n",
    "\n",
    "#         # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "#         A2, _ = forward_propagation2(X, parameters)\n",
    "\n",
    "#         # Função de custo. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "#         cost_new = compute_cost2(A2, Y, parameters)\n",
    "\n",
    "#         # Avaliar se a nova busca melhorou\n",
    "#         if cost_new < best_cost:\n",
    "#             parameters_best = parameters\n",
    "#             best_cost = cost_new\n",
    "\n",
    "#         # Print o custo (cost) a cada 5 iterações\n",
    "#         if print_cost and i % 2 == 0:\n",
    "#             print (\"Custo após iteração %i: %f\" %(i, cost_new))\n",
    "    \n",
    "#     return parameters_best, best_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experimento com as as séries do artigo \n",
    "\n",
    "## Importar funções para pre-processar os dados\n",
    "from funcoes import split_sequence, divisao_dados_temporais, normalizar_serie, desnormalizar, cenarios_dinamicos, cmf, gf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunspot\n",
    "\n",
    "Série anual.\n",
    "\n",
    "Inputs: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5.0\n",
       "1    11.0\n",
       "2    16.0\n",
       "3    23.0\n",
       "4    36.0\n",
       "Name: valor, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sunspot\n",
    "sunspot = pd.read_csv('dados/sunspot.csv')\n",
    "sunspot = sunspot['valor']\n",
    "sunspot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sunspot_norm = normalizar_serie(sunspot)\n",
    "X, y = split_sequence(sunspot_norm.values, 10, 1)\n",
    "\n",
    "X_treino, y_treino, X_teste, y_teste, X_val, y_val = divisao_dados_temporais(X, y, perc_treino = 0.56, perc_val = 0.24)\n",
    "# para avaliar mse\n",
    "y_teste_ = desnormalizar(y_teste, sunspot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Treinar rede neural com Rprop\n",
    "\n",
    "##### Testar 5 neurônios na camada escondida e 10 execuções\n",
    "best_model = 0\n",
    "best_mse = np.inf\n",
    "\n",
    "for exec in range(1):\n",
    "    print('Execução ' + str(exec))\n",
    "    parameters = nn_model_rprop(X_treino.T, y_treino.T, n_h=5, print_cost=True)\n",
    "    y_pred = predict2(parameters, X_val.T)\n",
    "    mse_exec = compute_cost2(y_pred,y_val.T, parameters)\n",
    "    \n",
    "    if mse_exec < best_mse:\n",
    "        best_model = parameters \n",
    "        best_mse = mse_exec\n",
    "\n",
    "    print('MSE validação = ' + str(mse_exec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Desnormalizar\n",
    "y_pred_teste = predict2(best_model, X_teste.T).reshape(-1,1)\n",
    "\n",
    "# mse de teste\n",
    "print(\"mse de teste = \",((y_pred_teste - y_teste)**2).mean())\n",
    "\n",
    "y_pred_teste = desnormalizar(y_pred_teste, sunspot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(y_teste_, color = 'blue')\n",
    "plt.plot(y_pred_teste, color = 'coral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Treinar rede neural com backprop\n",
    "\n",
    "##### Testar 4 neurônios na camada escondida e 10000 epocas com 30 execuções\n",
    "best_model = 0\n",
    "best_mse = np.inf\n",
    "\n",
    "#neuronios = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "neuronios = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for i in neuronios:\n",
    "    print('Neurônios: ', i)\n",
    "\n",
    "    for exec in np.arange(1):\n",
    "        print('Execução ' + str(exec))\n",
    "        parameters = nn_model2(X_treino.T, y_treino.T, n_h = i, print_cost = True)\n",
    "        y_pred = predict2(parameters, X_val.T)\n",
    "        mse_exec = compute_cost2(y_pred,y_val.T, parameters)\n",
    "    \n",
    "        if mse_exec < best_mse:\n",
    "            best_model = parameters \n",
    "            best_mse = mse_exec\n",
    "            print('Melhor MSE: ', best_mse)\n",
    "            qtd_neuronios = i\n",
    "\n",
    "    \n",
    "    print('MSE validação = ' + str(mse_exec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# melhor configuração\n",
    "print('NN com {} neurônios na camada escondida'.format(qtd_neuronios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Desnormalizar\n",
    "y_pred_teste = predict2(best_model, X_teste.T).reshape(-1,1)\n",
    "\n",
    "# mse de teste\n",
    "print(\"mse de teste = \",((y_pred_teste - y_teste)**2).mean())\n",
    "\n",
    "y_pred_teste = desnormalizar(y_pred_teste, sunspot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(y_teste_, label = 'Teste')\n",
    "plt.plot(y_pred_teste, color = 'coral', label = 'NN com Backprop')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Treinar rede neural com pso\n",
    "\n",
    "##### Testar 5 neurônios na camada escondida\n",
    "best_model = 0\n",
    "best_mse = np.inf\n",
    "\n",
    "neuronios = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "#neuronios = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for i in neuronios:\n",
    "    print('Neurônios: ', i)\n",
    "\n",
    "    for exec in np.arange(1):\n",
    "\n",
    "        print('Execução ' + str(exec))\n",
    "        parameters,_ = nn_model_pso_todos(X_treino.T, y_treino.T, n_h = i, print_cost = True)\n",
    "    \n",
    "        y_pred = predict2(parameters, X_val.T)\n",
    "        mse_exec = compute_cost2(y_pred, y_val.T,parameters)\n",
    "    \n",
    "        if mse_exec < best_mse:\n",
    "            best_model = parameters \n",
    "            \n",
    "            best_mse = mse_exec\n",
    "            print('Melhor MSE: ', best_mse)\n",
    "            qtd_neuronios = i\n",
    "\n",
    "        print('Neurônios' + str(i) + ' -> MSE validação = ' + str(mse_exec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('NN com {} neurônios na camada escondida'.format(qtd_neuronios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Desnormalizar\n",
    "y_pred_teste = predict2(best_model, X_teste.T).reshape(-1,1)\n",
    "\n",
    "# mse de teste\n",
    "print(\"mse de teste = \",((y_pred_teste - y_teste)**2).mean())\n",
    "\n",
    "y_pred_teste = desnormalizar(y_pred_teste, sunspot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.plot(y_teste_, label = 'Teste')\n",
    "plt.plot(y_pred_teste, label='NN com PSO')\n",
    "#plt.plot(y_teste_, color = 'blue', legend = 'Teste')\n",
    "#plt.plot(y_pred_teste, color = 'coral', legend = 'NN com PSO')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só previsão\n",
    "plt.plot(y_pred_teste, color = 'coral',label = 'NN com PSO')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cenário I - Sunspot\n",
    "\n",
    "Testando Cenário I\n",
    "\n",
    "* w = 60\n",
    "* s = 10\n",
    "* f = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando cenários\n",
    "def cenarios_execucoes(X, y, w, s, f, modelo, qtd_execucoes = 30):\n",
    "    \n",
    "    X_I = cenarios_dinamicos(X, 60, 10)\n",
    "    y_I = cenarios_dinamicos(y, 60, 10)\n",
    "\n",
    "    mse_treino = np.zeros((qtd_execucoes, len(y_I)))\n",
    "    mse_teste = np.zeros((qtd_execucoes, len(y_I)))\n",
    "\n",
    "    execucoes = np.arange(qtd_execucoes)\n",
    "\n",
    "    for execucao in execucoes:\n",
    "\n",
    "        print('Execução: ', execucao)\n",
    "    \n",
    "        # Janelamento\n",
    "        for i in np.arange(len(y_I)):\n",
    "    \n",
    "            ## Divisão em treinamento e teste\n",
    "            X_treino, y_treino, X_teste, y_teste, X_validacao, y_validacao = divisao_dados_temporais(X_I[i], y_I[i], perc_treino=.56, perc_val = .24)\n",
    "    \n",
    "            ### Treinar rede neural com backprop\n",
    "            # setando parâmetros para comparação\n",
    "            best_model = 0\n",
    "            best_mse = np.inf\n",
    "\n",
    "            # quantidade de neurônios de 2 até 25\n",
    "            neuronios = np.arange(2, 26)\n",
    "    \n",
    "            # grid search \n",
    "            for j in neuronios:\n",
    "                #print('Neurônios: ', j)\n",
    "        \n",
    "                # treinar NN para f iterações\n",
    "                parameters = modelo(X_treino.T, y_treino.T, n_h = j, num_iteracoes = f)\n",
    "        \n",
    "                # predição na validação\n",
    "                y_pred_val = predict2(parameters, X_validacao.T)\n",
    "                mse_validacao = compute_cost2(y_pred_val, y_validacao.T, parameters)\n",
    "        \n",
    "                if mse_validacao < best_mse:\n",
    "                    best_model = parameters \n",
    "                    best_mse = mse_validacao\n",
    "                    #print('Melhor MSE: ', best_mse)\n",
    "                    qtd_neuronios = j\n",
    "                \n",
    "            # retreinar e retestar com a melhor topologia \n",
    "            y_pred_treino = predict2(best_model, X_treino.T)\n",
    "            mse_treino_temp = compute_cost2(y_pred_treino, y_treino.T, best_model)\n",
    "            mse_treino[execucoes,i] = mse_treino_temp\n",
    "        \n",
    "            y_pred_teste = predict2(best_model, X_teste.T)\n",
    "            mse_teste_temp = compute_cost2(y_pred_teste, y_teste.T, best_model)\n",
    "            mse_teste[execucoes,i] = mse_teste_temp\n",
    "\n",
    "    return mse_treino, mse_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criando avaliação dos resultados\n",
    "def avaliacao_resultados(mse_treino, mse_teste, T):\n",
    "    \n",
    "    te = np.apply_along_axis(cmf, 1, mse_treino, T)\n",
    "    ge = np.apply_along_axis(cmf, 1, mse_teste, T)\n",
    "\n",
    "    # calcular a métrica fator de generalização\n",
    "    gf = ge/te\n",
    "\n",
    "    # Média e desvio padrão\n",
    "    te_medio = te.mean()\n",
    "    te_std = te.std()\n",
    "\n",
    "    ge_medio = ge.mean()\n",
    "    ge_std = ge.std()\n",
    "\n",
    "    gf_medio = gf.mean()\n",
    "    gf_std = gf.std()\n",
    "\n",
    "    print('TE medio: ', te_medio)\n",
    "    print('TE desvio: ', te_std)\n",
    "    print('GE medio: ', ge_medio)\n",
    "    print('GE desvio: ', ge_std)\n",
    "    print('GF medio: ', gf_medio)\n",
    "    print('GF desvio: ', gf_std)\n",
    "    \n",
    "    resultados = {'TE medio': te_medio,\n",
    "    'TE desvio': te_std,\n",
    "    'GE medio': ge_medio,\n",
    "    'GE desvio':ge_std,\n",
    "    'GF medio':gf_medio,\n",
    "    'GF desvio':gf_std}\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445.0\n"
     ]
    }
   ],
   "source": [
    "# Quantidade total de iterações para o primeiro cenário\n",
    "\n",
    "# Seed\n",
    "np.random.seed(3)\n",
    "\n",
    "w = 60 # tamanho da janela\n",
    "s = 10 # tamanho do passo\n",
    "f = 50 # quantidade de iterações para a janela\n",
    "T = f/s*(len(y))+f\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKPROP\n",
      "Execução:  0\n",
      "Execução:  1\n",
      "Execução:  2\n",
      "Execução:  3\n",
      "Execução:  4\n",
      "Execução:  5\n",
      "Execução:  6\n",
      "Execução:  7\n",
      "Execução:  8\n",
      "Execução:  9\n",
      "Execução:  10\n",
      "Execução:  11\n",
      "Execução:  12\n",
      "Execução:  13\n",
      "Execução:  14\n",
      "Execução:  15\n",
      "Execução:  16\n",
      "Execução:  17\n",
      "Execução:  18\n",
      "Execução:  19\n",
      "Execução:  20\n",
      "Execução:  21\n",
      "Execução:  22\n",
      "Execução:  23\n",
      "Execução:  24\n",
      "Execução:  25\n",
      "Execução:  26\n",
      "Execução:  27\n",
      "Execução:  28\n",
      "Execução:  29\n",
      "PSO\n",
      "Execução:  0\n",
      "Custo gbest inicio PSO =  1.101467096063771\n",
      "gbest 1\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0010802903633914008\n",
      "\n",
      "Custo menor que Gbest na iteração  2\n",
      "Custo =  0.0004435959829643932\n",
      "\n",
      "Custo menor que Gbest na iteração  17\n",
      "Custo =  0.0004212695703515198\n",
      "Erro de treinamento após otimizar parâmetros = 0.0004212695703515198\n",
      "Custo gbest inicio PSO =  0.16931949163622242\n",
      "gbest 8\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0007386455992493795\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004964923829972092\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00047307734174710763\n",
      "\n",
      "Custo menor que Gbest na iteração  25\n",
      "Custo =  0.0003557527772908293\n",
      "\n",
      "Custo menor que Gbest na iteração  29\n",
      "Custo =  0.0003301769598814753\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "Erro de treinamento após otimizar parâmetros = 0.0003234083949443378\n",
      "Custo gbest inicio PSO =  0.6668145940833247\n",
      "gbest 4\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0015652347593757708\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0012975599151355004\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004475030851892056\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  18\n",
      "Custo =  0.00040746236077537267\n",
      "\n",
      "Custo menor que Gbest na iteração  32\n",
      "Custo =  0.00031141184453731155\n",
      "Erro de treinamento após otimizar parâmetros = 0.00031141184453731155\n",
      "Custo gbest inicio PSO =  0.029848214052954875\n",
      "gbest 21\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0025347796100411353\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004776757250396588\n",
      "\n",
      "Custo menor que Gbest na iteração  9\n",
      "Custo =  0.00046015636545831544\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  29\n",
      "Custo =  0.0004300324056377663\n",
      "Erro de treinamento após otimizar parâmetros = 0.0004300324056377663\n",
      "Custo gbest inicio PSO =  0.4139849701829034\n",
      "gbest 42\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0026171010110148597\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0022513635493486787\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0021904995453235023\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0006472705610960873\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0005055660138730928\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00044900455128829054\n",
      "\n",
      "Custo menor que Gbest na iteração  15\n",
      "Custo =  0.00024371147335047424\n",
      "\n",
      "Custo menor que Gbest na iteração  17\n",
      "Custo =  0.00021893617967095905\n",
      "Erro de treinamento após otimizar parâmetros = 0.00021893617967095905\n",
      "Custo gbest inicio PSO =  1.0986672611683552\n",
      "gbest 1\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0006498067409554158\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00044777745254805036\n",
      "\n",
      "Custo menor que Gbest na iteração  31\n",
      "Custo =  0.00043291709214274434\n",
      "\n",
      "Custo menor que Gbest na iteração  33\n",
      "Custo =  0.0003455910138984187\n",
      "\n",
      "Custo menor que Gbest na iteração  39\n",
      "Custo =  0.00031297683895827223\n",
      "\n",
      "Custo menor que Gbest na iteração  47\n",
      "Custo =  0.0003017152548233379\n",
      "Erro de treinamento após otimizar parâmetros = 0.0003017152548233379\n",
      "Custo gbest inicio PSO =  0.004598303928101358\n",
      "gbest 81\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0024992214018193506\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0020393929155946592\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0008513533182534774\n",
      "\n",
      "Custo menor que Gbest na iteração  8\n",
      "Custo =  0.0006067935814922065\n",
      "\n",
      "Custo menor que Gbest na iteração  16\n",
      "Custo =  0.00048406519208107733\n",
      "\n",
      "Custo menor que Gbest na iteração  22\n",
      "Custo =  0.00034686400022521563\n",
      "\n",
      "Custo menor que Gbest na iteração  32\n",
      "Custo =  0.00023897137652972676\n",
      "\n",
      "Custo menor que Gbest na iteração  35\n",
      "Custo =  0.000157990123979911\n",
      "Erro de treinamento após otimizar parâmetros = 0.000157990123979911\n",
      "Custo gbest inicio PSO =  1.0586966611194286\n",
      "gbest 47\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0007619227721753166\n",
      "\n",
      "Custo menor que Gbest na iteração  11\n",
      "Custo =  0.0005863303163903804\n",
      "\n",
      "Custo menor que Gbest na iteração  14\n",
      "Custo =  0.0004037338918526516\n",
      "\n",
      "Custo menor que Gbest na iteração  21\n",
      "Custo =  0.0003860869876336239\n",
      "\n",
      "Custo menor que Gbest na iteração  23\n",
      "Custo =  0.00022378398158633573\n",
      "Erro de treinamento após otimizar parâmetros = 0.00022378398158633573\n",
      "Custo gbest inicio PSO =  0.35027134815404254\n",
      "gbest 83\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0010401308998327762\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0008593984435612736\n",
      "\n",
      "Custo menor que Gbest na iteração  4\n",
      "Custo =  0.0006935843610066155\n",
      "\n",
      "Custo menor que Gbest na iteração  5\n",
      "Custo =  0.0005794871729032871\n",
      "\n",
      "Custo menor que Gbest na iteração  12\n",
      "Custo =  0.00039474461628949165\n",
      "\n",
      "Custo menor que Gbest na iteração  20\n",
      "Custo =  0.00013417844812388028\n",
      "\n",
      "Custo menor que Gbest na iteração  39\n",
      "Custo =  9.816392416383904e-05\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "Erro de treinamento após otimizar parâmetros = 8.891382382570264e-05\n",
      "Custo gbest inicio PSO =  0.01582327403844477\n",
      "gbest 66\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004476322716886233\n",
      "\n",
      "Custo menor que Gbest na iteração  8\n",
      "Custo =  0.00030417611428505155\n",
      "\n",
      "Custo menor que Gbest na iteração  23\n",
      "Custo =  0.00019769876631249437\n",
      "\n",
      "Custo menor que Gbest na iteração  37\n",
      "Custo =  0.0001295662604094325\n",
      "\n",
      "Custo menor que Gbest na iteração  39\n",
      "Custo =  9.500329527313142e-05\n",
      "Erro de treinamento após otimizar parâmetros = 9.500329527313142e-05\n",
      "Custo gbest inicio PSO =  1.1729981225148631\n",
      "gbest 91\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.002675275647873783\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0011107608809457345\n",
      "\n",
      "Custo menor que Gbest na iteração  2\n",
      "Custo =  0.0007613931467780153\n",
      "\n",
      "Custo menor que Gbest na iteração  5\n",
      "Custo =  0.00039843466785684337\n",
      "\n",
      "Custo menor que Gbest na iteração  31\n",
      "Custo =  0.00029667208487673655\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  37\n",
      "Custo =  0.0002706086397187512\n",
      "Erro de treinamento após otimizar parâmetros = 0.0002706086397187512\n",
      "Custo gbest inicio PSO =  0.523356525866345\n",
      "gbest 153\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0013220792582541835\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004517767087684581\n",
      "\n",
      "Custo menor que Gbest na iteração  13\n",
      "Custo =  0.0003816308715518585\n",
      "\n",
      "Custo menor que Gbest na iteração  21\n",
      "Custo =  0.0002708965322633125\n",
      "\n",
      "Custo menor que Gbest na iteração  46\n",
      "Custo =  0.00022102564385255555\n",
      "Erro de treinamento após otimizar parâmetros = 0.00022102564385255555\n",
      "Custo gbest inicio PSO =  0.4048109628701257\n",
      "gbest 145\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0005185916913720355\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  7\n",
      "Custo =  0.0004404685058801676\n",
      "\n",
      "Custo menor que Gbest na iteração  12\n",
      "Custo =  0.00033242680326337116\n",
      "\n",
      "Custo menor que Gbest na iteração  15\n",
      "Custo =  0.00028839563324664777\n",
      "\n",
      "Custo menor que Gbest na iteração  28\n",
      "Custo =  0.0001411450668460181\n",
      "Erro de treinamento após otimizar parâmetros = 0.0001411450668460181\n",
      "Custo gbest inicio PSO =  0.3266817664189632\n",
      "gbest 52\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.002806559760044424\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0020413817133568095\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0011644254270184347\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00068441130185573\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004096849690334875\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00036868705569249275\n",
      "\n",
      "Custo menor que Gbest na iteração  10\n",
      "Custo =  0.0003536506202918043\n",
      "\n",
      "Custo menor que Gbest na iteração  10\n",
      "Custo =  0.00021597686263554255\n",
      "\n",
      "Custo menor que Gbest na iteração  38\n",
      "Custo =  0.00013823090010746706\n",
      "\n",
      "Custo menor que Gbest na iteração  39\n",
      "Custo =  9.194439474136382e-05\n",
      "Erro de treinamento após otimizar parâmetros = 9.194439474136382e-05\n",
      "Custo gbest inicio PSO =  0.1136789781897467\n",
      "gbest 155\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0007733454536393456\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0005734907321445422\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0005145754268768275\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0003787488174456931\n",
      "\n",
      "Custo menor que Gbest na iteração  9\n",
      "Custo =  0.0002763548443289528\n",
      "\n",
      "Custo menor que Gbest na iteração  20\n",
      "Custo =  0.0002520271571286505\n",
      "\n",
      "Custo menor que Gbest na iteração  22\n",
      "Custo =  0.00023238688063216594\n",
      "\n",
      "Custo menor que Gbest na iteração  29\n",
      "Custo =  0.00018724729709876793\n",
      "Erro de treinamento após otimizar parâmetros = 0.00018724729709876793\n",
      "Custo gbest inicio PSO =  0.7758148194529535\n",
      "gbest 66\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.004475666134721384\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.00046582787370943237\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0004042897084863676\n",
      "\n",
      "Custo menor que Gbest na iteração  5\n",
      "Custo =  0.0003065238765977395\n",
      "\n",
      "Custo menor que Gbest na iteração  28\n",
      "Custo =  0.00027862809531744\n",
      "\n",
      "Custo menor que Gbest na iteração  34\n",
      "Custo =  0.000267442215572176\n",
      "\n",
      "Custo menor que Gbest na iteração  34\n",
      "Custo =  0.00024502891508547387\n",
      "\n",
      "Custo menor que Gbest na iteração  39\n",
      "Custo =  0.00014977695091690415\n",
      "Erro de treinamento após otimizar parâmetros = 0.00014977695091690415\n",
      "Custo gbest inicio PSO =  0.521656222391417\n",
      "gbest 5\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.0007922596924390674\n",
      "\n",
      "Custo menor que Gbest na iteração  1\n",
      "Custo =  0.000551465211870564\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  5\n",
      "Custo =  0.00047063014001797897\n",
      "\n",
      "Custo menor que Gbest na iteração  17\n",
      "Custo =  0.00044028534562257776\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  36\n",
      "Custo =  0.0004246287826279099\n",
      "\n",
      "Custo menor que Gbest na iteração  37\n",
      "Custo =  0.000366572178424656\n",
      "Parar o algoritmo pois o gbest não melhorou\n",
      "\n",
      "Custo menor que Gbest na iteração  46\n",
      "Custo =  0.00023921133764845398\n",
      "Parar o algoritmo pois o gbest não melhorou\n"
     ]
    }
   ],
   "source": [
    "# backprop\n",
    "print('BACKPROP')\n",
    "mse_treino_1_backprop, mse_teste_1_backprop = cenarios_execucoes(X, y, w, s, f, modelo = nn_model2)\n",
    "#resultados_1_backprop = avaliacao_resultados(mse_treino_1_backprop, mse_teste_1_backprop, T) \n",
    "\n",
    "print('PSO')\n",
    "# pso\n",
    "mse_treino_1_pso, mse_teste_1_pso = cenarios_execucoes(X, y, w, s, f, modelo = nn_model_pso_todos)\n",
    "resultados_1_pso = avaliacao_resultados(mse_treino_1_pso, mse_teste_1_pso, T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cenário II - Sunspot\n",
    "\n",
    "Testando Cenário II\n",
    "\n",
    "* w = 60\n",
    "* s = 20\n",
    "* f = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 60 # tamanho da janela\n",
    "s = 20 # tamanho do passo\n",
    "f = 100 # quantidade de iterações para a janela\n",
    "T = f/s*(len(y))+f\n",
    "print('Quantidade total de iterações: ', T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop\n",
    "print('BACKPROP')\n",
    "mse_treino_2_backprop, mse_teste_2_backprop = cenarios_execucoes(X, y, w, s, f, modelo = nn_model2)\n",
    "resultados_2_backprop = avaliacao_resultados(mse_treino_2_backprop, mse_teste_2_backprop, T) \n",
    "\n",
    "print('PSO')\n",
    "# pso\n",
    "mse_treino_2_pso, mse_teste_2_pso = cenarios_execucoes(X, y, w, s, f, modelo = nn_model_pso_todos)\n",
    "resultados_2_pso = avaliacao_resultados(mse_treino_2_pso, mse_teste_2_pso, T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cenário III - Sunspot\n",
    "\n",
    "Testando Cenário III\n",
    "\n",
    "* w = 60\n",
    "* s = 40\n",
    "* f = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 60 # tamanho da janela\n",
    "s = 40 # tamanho do passo\n",
    "f = 150 # quantidade de iterações para a janela\n",
    "T = f/s*(len(y))+f\n",
    "print('Quantidade total de iterações: ', T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop\n",
    "print('BACKPROP')\n",
    "mse_treino_3_backprop, mse_teste_3_backprop = cenarios_execucoes(X, y, w, s, f, modelo = nn_model2)\n",
    "resultados_3_backprop = avaliacao_resultados(mse_treino_3_backprop, mse_teste_3_backprop, T) \n",
    "\n",
    "print('PSO')\n",
    "# pso\n",
    "mse_treino_3_pso, mse_teste_3_pso = cenarios_execucoes(X, y, w, s, f, modelo = nn_model_pso_todos)\n",
    "resultados_3_pso = avaliacao_resultados(mse_treino_3_pso, mse_teste_3_pso, T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cenário IV - Sunspot\n",
    "\n",
    "Testando Cenário IV\n",
    "\n",
    "* w = 60\n",
    "* s = 60\n",
    "* f = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 60 # tamanho da janela\n",
    "s = 60 # tamanho do passo\n",
    "f = 100 # quantidade de iterações para a janela\n",
    "T = f/s*(len(y))+f\n",
    "print('Quantidade total de iterações: ', T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop\n",
    "print('BACKPROP')\n",
    "mse_treino_4_backprop, mse_teste_4_backprop = cenarios_execucoes(X, y, w, s, f, modelo = nn_model2)\n",
    "resultados_4_backprop = avaliacao_resultados(mse_treino_4_backprop, mse_teste_4_backprop, T) \n",
    "\n",
    "print('PSO')\n",
    "# pso\n",
    "mse_treino_4_pso, mse_teste_4_pso = cenarios_execucoes(X, y, w, s, f, modelo = nn_model_pso_todos)\n",
    "resultados_4_pso = avaliacao_resultados(mse_treino_4_pso, mse_teste_4_pso, T) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Time Series\n",
    "\n",
    "Série mensal\n",
    "Inputs: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Airline Passenger\n",
    "airline = pd.read_csv('dados/airline_passengers.csv')\n",
    "airline = airline['valor']\n",
    "airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
